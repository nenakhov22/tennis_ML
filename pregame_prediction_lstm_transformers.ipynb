{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69a08986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42242"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "atp_matches_2024 = pd.read_csv('atp_matches_2024.csv')\n",
    "atp_matches_2023 = pd.read_csv('atp_matches_2023.csv')\n",
    "atp_matches_2022 = pd.read_csv('atp_matches_2022.csv')\n",
    "\n",
    "atp_matches_qual_chall_2023 = pd.read_csv('atp_matches_qual_chall_2023.csv')\n",
    "atp_matches_qual_chall_2024 = pd.read_csv('atp_matches_qual_chall_2024.csv')\n",
    "atp_matches_qual_chall_2022 = pd.read_csv('atp_matches_qual_chall_2022.csv')\n",
    "\n",
    "df = pd.concat([atp_matches_2024,atp_matches_2023, atp_matches_qual_chall_2024, atp_matches_qual_chall_2023, atp_matches_2022, atp_matches_qual_chall_2022], ignore_index=True)\n",
    "\n",
    "df['tourney_name'] = df['tourney_name'].apply(lambda x: 'Davis Cup' if x.startswith('Davis Cup') else x)\n",
    "df = df[df['tourney_name']!='Davis Cup']\n",
    "df = df[df['tourney_name']!='United Cup']\n",
    "df = df[df['surface'].isin(['Hard','Clay','Grass'])]\n",
    "df = df[df['best_of'] == 3]\n",
    "df = df[df['winner_rank'].notnull() & df['loser_rank'].notnull() & df['winner_hand'].notnull() & df['loser_hand'].notnull()]\n",
    "\n",
    "# --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---\n",
    "# —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ –∏ –Ω–æ–º–µ—Ä—É –º–∞—Ç—á–∞\n",
    "df = df.sort_values(by=[\"tourney_date\", \"match_num\"]).reset_index(drop=True)\n",
    "\n",
    "# –£–Ω–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã (–≤ int –º–æ–∂–µ—Ç –±—ã—Ç—å YYYYMMDD)\n",
    "df[\"tourney_date\"] = pd.to_datetime(df[\"tourney_date\"], format=\"%Y%m%d\")\n",
    "\n",
    "# –£–±–∏—Ä–∞–µ–º –º–∞—Ç—á–∏ –±–µ–∑ –ø–æ–¥–∞—á\n",
    "df = df[(df[\"w_svpt\"] > 0) & (df[\"l_svpt\"] > 0) & (df[\"w_1stIn\"] > 0) & (df[\"l_1stIn\"] > 0)]\n",
    "\n",
    "\n",
    "\n",
    "def extract_relative_features(row, player=\"winner\"):\n",
    "    \"\"\"\n",
    "    row: —Å—Ç—Ä–æ–∫–∞ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ (–æ–¥–∏–Ω –º–∞—Ç—á)\n",
    "    player: 'winner' –∏–ª–∏ 'loser' (–¥–ª—è –∫–æ–≥–æ —Å—á–∏—Ç–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é)\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–Ω–∏—Ü—É —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–≥—Ä–æ–∫–∞ –∏ –µ–≥–æ —Å–æ–ø–µ—Ä–Ω–∏–∫–∞\n",
    "    \"\"\"\n",
    "    if player == \"winner\":\n",
    "        p, o = \"w_\", \"l_\"\n",
    "        pr, or_ = \"winner\", \"loser\"\n",
    "    else:\n",
    "        p, o = \"l_\", \"w_\"\n",
    "        pr, or_ = \"loser\", \"winner\"\n",
    "    \n",
    "    feats = {}\n",
    "    # –≠–π—Å—ã –∏ –¥–≤–æ–π–Ω—ã–µ\n",
    "    feats[\"aces_diff\"] = row[f\"{p}ace\"] - row[f\"{o}ace\"]\n",
    "    feats[\"df_diff\"] = row[f\"{p}df\"] - row[f\"{o}df\"]\n",
    "\n",
    "    # % –ø–µ—Ä–≤–æ–π –ø–æ–¥–∞—á–∏\n",
    "    feats[\"first_in_diff\"] = (row[f\"{p}1stIn\"]/row[f\"{p}svpt\"]) - \\\n",
    "                             (row[f\"{o}1stIn\"]/row[f\"{o}svpt\"])\n",
    "\n",
    "    # % –æ—á–∫–æ–≤ –Ω–∞ –ø–µ—Ä–≤–æ–π –ø–æ–¥–∞—á–µ\n",
    "    feats[\"first_won_diff\"] = (row[f\"{p}1stWon\"]/row[f\"{p}1stIn\"]) - \\\n",
    "                              (row[f\"{o}1stWon\"]/row[f\"{o}1stIn\"])\n",
    "\n",
    "    def safe_div(num, denom):\n",
    "        return num / denom if denom != 0 else 0.0\n",
    "\n",
    "    # –ø—Ä–æ—Ü–µ–Ω—Ç –≤—ã–∏–≥—Ä–∞–Ω–Ω—ã—Ö –≤—Ç–æ—Ä—ã—Ö –ø–æ–¥–∞—á\n",
    "    feats[\"second_won_diff\"] = safe_div(row[f\"{p}2ndWon\"], row[f\"{p}svpt\"] - row[f\"{p}1stIn\"]) - \\\n",
    "                            safe_div(row[f\"{o}2ndWon\"], row[f\"{o}svpt\"] - row[f\"{o}1stIn\"])\n",
    "\n",
    "\n",
    "    # % –æ—Ç–±–∏—Ç—ã—Ö –±—Ä–µ–π–∫–æ–≤\n",
    "    # –ø—Ä–æ—Ü–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –±—Ä–µ–π–∫–ø–æ–∏–Ω—Ç–æ–≤ (bpSaved / bpFaced)\n",
    "    def safe_bp_ratio(saved, faced):\n",
    "        return saved / faced if faced > 0 else 0.0\n",
    "\n",
    "    feats[\"bp_saved_diff\"] = safe_bp_ratio(row[f\"{p}bpSaved\"], row[f\"{p}bpFaced\"]) - \\\n",
    "                            safe_bp_ratio(row[f\"{o}bpSaved\"], row[f\"{o}bpFaced\"])\n",
    "\n",
    "    # —Å–∫–æ–ª—å–∫–æ –±—ã–ª–æ –±—Ä–µ–π–∫–ø–æ–∏–Ω—Ç–æ–≤\n",
    "    feats[\"bp_diff\"] = row[f\"{p}bpFaced\"] - row[f\"{o}bpFaced\"]\n",
    "\n",
    "    # üéØ —Ä–∞–∑–Ω–∏—Ü–∞ –ø–æ –æ–±—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ—á–∫–æ–≤ (Total points won)\n",
    "    player_points = row[f\"{p}1stWon\"] + row[f\"{p}2ndWon\"]\n",
    "    opp_points = row[f\"{o}1stWon\"] + row[f\"{o}2ndWon\"]\n",
    "    feats[\"points_diff\"] = player_points - opp_points\n",
    "\n",
    "    return feats\n",
    "\n",
    "\n",
    "# –ü–æ—Å—Ç—Ä–æ–∏–º –∏—Å—Ç–æ—Ä–∏—é –º–∞—Ç—á–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–≥—Ä–æ–∫–∞\n",
    "player_history = {}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    for pl in [\"winner\", \"loser\"]:\n",
    "        pid = row[f\"{pl}_id\"]\n",
    "        feats = extract_relative_features(row, pl)\n",
    "        feats[\"date\"] = row[\"tourney_date\"]\n",
    "        feats[\"result\"] = 1 if pl == \"winner\" else 0  # —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–∞—Ç—á–∞ –¥–ª—è —ç—Ç–æ–≥–æ –∏–≥—Ä–æ–∫–∞\n",
    "        \n",
    "        if pid not in player_history:\n",
    "            player_history[pid] = []\n",
    "        player_history[pid].append(feats)\n",
    "\n",
    "\n",
    "# Dataset –¥–ª—è Siamese LSTM\n",
    "class TennisSiameseDataset(Dataset):\n",
    "    def __init__(self, matches_df, player_history, n_matches=5):\n",
    "        self.matches_df = matches_df\n",
    "        self.player_history = player_history\n",
    "        self.n_matches = n_matches\n",
    "        self.samples = []\n",
    "        \n",
    "        for idx, row in matches_df.iterrows():\n",
    "            p1, p2 = row[\"winner_id\"], row[\"loser_id\"]\n",
    "            label = 1  # –ø–æ–±–µ–¥–∞ –∏–≥—Ä–æ–∫–∞ 1 (winner)\n",
    "            \n",
    "            # –ò—Å—Ç–æ—Ä–∏—è\n",
    "            hist1 = [h for h in player_history[p1] if h[\"date\"] < row[\"tourney_date\"]]\n",
    "            hist2 = [h for h in player_history[p2] if h[\"date\"] < row[\"tourney_date\"]]\n",
    "            \n",
    "            if len(hist1) >= self.n_matches and len(hist2) >= self.n_matches:\n",
    "                # –ø–æ–±–µ–¥–∏—Ç–µ–ª—å (label=1)\n",
    "                self.samples.append((p1, p2, row[\"tourney_date\"], 1, row))\n",
    "                # –ø—Ä–æ–∏–≥—Ä–∞–≤—à–∏–π (label=0) ‚Äî –∑–µ—Ä–∫–∞–ª—å–Ω–æ\n",
    "                self.samples.append((p2, p1, row[\"tourney_date\"], 0, row))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        p1, p2, date, label, row = self.samples[idx]\n",
    "        hist1 = [h for h in self.player_history[p1] if h[\"date\"] < date][-self.n_matches:]\n",
    "        hist2 = [h for h in self.player_history[p2] if h[\"date\"] < date][-self.n_matches:]\n",
    "            \n",
    "        def to_vec_list(hist):\n",
    "            feat_names = [k for k in hist[0].keys() if k not in [\"date\",\"result\"]]\n",
    "            arr = []\n",
    "            for h in hist:\n",
    "                arr.append([h[f] for f in feat_names])\n",
    "            # –ø–∞–¥–¥–∏–Ω–≥, –µ—Å–ª–∏ –º–µ–Ω—å—à–µ n_matches\n",
    "            while len(arr) < self.n_matches:\n",
    "                arr.insert(0, [0]*len(feat_names))\n",
    "            return np.array(arr, dtype=np.float32)\n",
    "            \n",
    "        X1 = to_vec_list(hist1)\n",
    "        X2 = to_vec_list(hist2)\n",
    "\n",
    "        # üîë —Å—Ä–∞–∑—É –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã\n",
    "        X1 = torch.tensor(X1, dtype=torch.float32)\n",
    "        X2 = torch.tensor(X2, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)  # –µ—Å–ª–∏ BCEWithLogitsLoss\n",
    "        # label = torch.tensor(label, dtype=torch.long)   # –µ—Å–ª–∏ CrossEntropyLoss\n",
    "            \n",
    "        # üéæ –º–∞—Ç—á–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "        match_feats = torch.tensor([\n",
    "            row[\"winner_rank\"] if p1 == row[\"winner_id\"] else row[\"loser_rank\"],\n",
    "            row[\"loser_rank\"] if p2 == row[\"loser_id\"] else row[\"winner_rank\"],\n",
    "            int(row[\"winner_hand\"] == row[\"loser_hand\"])\n",
    "        ], dtype=torch.float32) \n",
    "            \n",
    "        return X1, X2, label, match_feats\n",
    "\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "dataset = TennisSiameseDataset(df, player_history, n_matches=10)\n",
    "len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "332ad015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# === 1. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test ===\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "all_train_features = []\n",
    "all_match_features = []\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    X1, X2, _, match_feats = train_dataset[i]\n",
    "    all_train_features.append(X1)\n",
    "    all_train_features.append(X2)\n",
    "    all_match_features.append(match_feats)\n",
    "\n",
    "# –ö–æ–Ω–∫–∞—Ç–∏–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "all_train_features = torch.cat(all_train_features, dim=0)  # [N*seq_len, n_feats]\n",
    "all_match_features = torch.stack(all_match_features, dim=0)  # [N, n_match_feats]\n",
    "\n",
    "# –°—á–∏—Ç–∞–µ–º mean/std\n",
    "mean_seq = all_train_features.mean(dim=0, keepdim=True)\n",
    "std_seq = all_train_features.std(dim=0, keepdim=True)\n",
    "\n",
    "mean_match = all_match_features.mean(dim=0, keepdim=True)\n",
    "std_match = all_match_features.std(dim=0, keepdim=True)\n",
    "\n",
    "# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "def normalize_dataset(ds, mean_seq, std_seq, mean_match, std_match):\n",
    "    normed = []\n",
    "    for i in range(len(ds)):\n",
    "        X1, X2, y, match_feats = ds[i]\n",
    "        X1 = (X1 - mean_seq) / (std_seq + 1e-10)\n",
    "        X2 = (X2 - mean_seq) / (std_seq + 1e-10)\n",
    "        match_feats = (match_feats - mean_match) / (std_match + 1e-10)\n",
    "        normed.append((X1, X2, y, match_feats))\n",
    "    return normed\n",
    "\n",
    "train_data = normalize_dataset(train_dataset, mean_seq, std_seq, mean_match, std_match)\n",
    "test_data  = normalize_dataset(test_dataset, mean_seq, std_seq, mean_match, std_match)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30c2c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. –ú–æ–¥–µ–ª—å Siamese LSTM ===\n",
    "class SiameseLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1, match_dim=0, use_match_feats=True):\n",
    "        super().__init__()\n",
    "        self.use_match_feats = use_match_feats\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        if self.use_match_feats:\n",
    "            self.fc1 = nn.Linear(hidden_dim*2 + match_dim, 1)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(hidden_dim*2, 1)\n",
    "\n",
    "        #self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        #_, h = self.lstm(x)\n",
    "        return h[-1]\n",
    "\n",
    "    def forward(self, x1, x2, match_feats=None):\n",
    "        h1 = self.forward_once(x1)\n",
    "        h2 = self.forward_once(x2)\n",
    "\n",
    "        if self.use_match_feats:\n",
    "            if match_feats.dim() == 3:\n",
    "                match_feats = match_feats.squeeze(1)\n",
    "            combined = torch.cat([h1, h2, match_feats], dim=1)\n",
    "            #x = torch.relu(self.fc1(combined))\n",
    "            return self.fc1(combined).squeeze(-1)\n",
    "        else:\n",
    "            combined = torch.cat([h1, h2], dim=1)\n",
    "            return self.fc1(combined).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "# === 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ ===\n",
    "sample_X1, sample_X2, _, sample_match_feats = dataset[0]\n",
    "input_dim = sample_X1.shape[1]\n",
    "match_dim = sample_match_feats.shape[0]\n",
    "\n",
    "model = SiameseLSTM(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=32,\n",
    "    match_dim=match_dim,\n",
    "    use_match_feats=True  # match_dim –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å 0\n",
    ")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f14f8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SiameseTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=32, num_heads=2, num_layers=1, match_dim=3, max_len=30):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # –ª–∏–Ω–µ–π–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # CLS —Ç–æ–∫–µ–Ω\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
    "\n",
    "        # –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π —ç–Ω–∫–æ–¥–∏–Ω–≥ (–æ–±—É—á–∞–µ–º—ã–π)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len+1, hidden_dim))\n",
    "\n",
    "        # —ç–Ω–∫–æ–¥–µ—Ä\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim*2,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim*2 + match_dim, 1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # x: [batch, seq_len, input_dim]\n",
    "        b, seq_len, _ = x.size()\n",
    "\n",
    "        # –ø—Ä–æ–µ–∫—Ü–∏—è –≤ —Å–∫—Ä—ã—Ç–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        # CLS —Ç–æ–∫–µ–Ω\n",
    "        cls_tokens = self.cls_token.expand(b, -1, -1)  # [b, 1, hidden_dim]\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "\n",
    "        # –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π —ç–Ω–∫–æ–¥–∏–Ω–≥\n",
    "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
    "\n",
    "        # —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä\n",
    "        out = self.transformer(x)\n",
    "\n",
    "        # –±–µ—Ä—ë–º CLS\n",
    "        return self.norm(out[:, 0, :])  # [b, hidden_dim]\n",
    "\n",
    "    def forward(self, x1, x2, match_feats):\n",
    "        z1 = self.encode(x1)\n",
    "        z2 = self.encode(x2)\n",
    "\n",
    "        if match_feats.dim() == 3:\n",
    "            match_feats = match_feats.squeeze(1)\n",
    "\n",
    "        z = torch.cat([z1, z2, match_feats], dim=1)\n",
    "        return self.fc(z).squeeze(-1)\n",
    "        \n",
    "# === 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ ===\n",
    "sample_X1, sample_X2, _, sample_match_feats = dataset[0]\n",
    "input_dim = sample_X1.shape[1]\n",
    "match_dim = sample_match_feats.shape[0]\n",
    "model = SiameseTransformer(\n",
    "    input_dim=input_dim\n",
    ")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11221802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def sinusoidal_positional_encoding(max_len, hidden_dim):\n",
    "    pe = torch.zeros(max_len, hidden_dim)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * -(math.log(10000.0) / hidden_dim))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe.unsqueeze(0)  # [1, max_len, hidden_dim]\n",
    "\n",
    "\n",
    "class SiameseTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=32, num_heads=2, num_layers=1, match_dim=3, max_len=30):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # –ª–∏–Ω–µ–π–Ω–∞—è –ø—Ä–æ–µ–∫—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ sin/cos –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "        pe = sinusoidal_positional_encoding(max_len, hidden_dim)\n",
    "        self.register_buffer(\"pos_embedding\", pe, persistent=False)\n",
    "\n",
    "        # —ç–Ω–∫–æ–¥–µ—Ä\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 2,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 + match_dim, 1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # x: [batch, seq_len, input_dim]\n",
    "        b, seq_len, _ = x.size()\n",
    "\n",
    "        # –ø—Ä–æ–µ–∫—Ü–∏—è\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        # –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–π —ç–Ω–∫–æ–¥–∏–Ω–≥\n",
    "        pos_emb = self.pos_embedding[:, :seq_len, :]\n",
    "        x = x + pos_emb\n",
    "\n",
    "        # —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä\n",
    "        out = self.transformer(x)\n",
    "\n",
    "        # mean pooling –ø–æ —Ç–æ–∫–µ–Ω–∞–º\n",
    "        pooled, _ = out.max(dim=1)\n",
    "\n",
    "        return self.norm(pooled)  # [b, hidden_dim]\n",
    "\n",
    "    def forward(self, x1, x2, match_feats):\n",
    "        z1 = self.encode(x1)\n",
    "        z2 = self.encode(x2)\n",
    "\n",
    "        if match_feats.dim() == 3:\n",
    "            match_feats = match_feats.squeeze(1)\n",
    "\n",
    "        z = torch.cat([z1, z2, match_feats], dim=1)\n",
    "        return self.fc(z).squeeze(-1)\n",
    "\n",
    "model = SiameseTransformer(\n",
    "    input_dim=input_dim\n",
    ")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4984f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6737\n",
      "Epoch 01 | Train Loss: 0.6737 | Val Loss: 0.6655 | LR: 0.00100\n",
      "Epoch 2, Loss: 0.6614\n",
      "Epoch 02 | Train Loss: 0.6614 | Val Loss: 0.6587 | LR: 0.00100\n",
      "Epoch 3, Loss: 0.6603\n",
      "Epoch 03 | Train Loss: 0.6603 | Val Loss: 0.6559 | LR: 0.00100\n",
      "Epoch 4, Loss: 0.6589\n",
      "Epoch 04 | Train Loss: 0.6589 | Val Loss: 0.6549 | LR: 0.00100\n",
      "Epoch 5, Loss: 0.6576\n",
      "Epoch 05 | Train Loss: 0.6576 | Val Loss: 0.6560 | LR: 0.00100\n",
      "Epoch 6, Loss: 0.6560\n",
      "Epoch 06 | Train Loss: 0.6560 | Val Loss: 0.6547 | LR: 0.00100\n",
      "Epoch 7, Loss: 0.6556\n",
      "Epoch 07 | Train Loss: 0.6556 | Val Loss: 0.6559 | LR: 0.00100\n",
      "Epoch 8, Loss: 0.6539\n",
      "Epoch 08 | Train Loss: 0.6539 | Val Loss: 0.6553 | LR: 0.00100\n",
      "Epoch 9, Loss: 0.6523\n",
      "Epoch 09 | Train Loss: 0.6523 | Val Loss: 0.6549 | LR: 0.00100\n",
      "Epoch 10, Loss: 0.6511\n",
      "Epoch 10 | Train Loss: 0.6511 | Val Loss: 0.6520 | LR: 0.00100\n",
      "Epoch 11, Loss: 0.6486\n",
      "Epoch 11 | Train Loss: 0.6486 | Val Loss: 0.6543 | LR: 0.00100\n",
      "Epoch 12, Loss: 0.6454\n",
      "Epoch 12 | Train Loss: 0.6454 | Val Loss: 0.6526 | LR: 0.00100\n",
      "Epoch 13, Loss: 0.6443\n",
      "Epoch 13 | Train Loss: 0.6443 | Val Loss: 0.6528 | LR: 0.00100\n",
      "Epoch 14, Loss: 0.6437\n",
      "Epoch 14 | Train Loss: 0.6437 | Val Loss: 0.6531 | LR: 0.00100\n",
      "Epoch 15, Loss: 0.6415\n",
      "Epoch 15 | Train Loss: 0.6415 | Val Loss: 0.6506 | LR: 0.00100\n",
      "Epoch 16, Loss: 0.6389\n",
      "Epoch 16 | Train Loss: 0.6389 | Val Loss: 0.6522 | LR: 0.00100\n",
      "Epoch 17, Loss: 0.6392\n",
      "Epoch 17 | Train Loss: 0.6392 | Val Loss: 0.6507 | LR: 0.00100\n",
      "Epoch 18, Loss: 0.6367\n",
      "Epoch 18 | Train Loss: 0.6367 | Val Loss: 0.6490 | LR: 0.00100\n",
      "Epoch 19, Loss: 0.6344\n",
      "Epoch 19 | Train Loss: 0.6344 | Val Loss: 0.6488 | LR: 0.00100\n",
      "Epoch 20, Loss: 0.6308\n",
      "Epoch 20 | Train Loss: 0.6308 | Val Loss: 0.6525 | LR: 0.00100\n",
      "Epoch 21, Loss: 0.6315\n",
      "Epoch 21 | Train Loss: 0.6315 | Val Loss: 0.6482 | LR: 0.00100\n",
      "Epoch 22, Loss: 0.6277\n",
      "Epoch 22 | Train Loss: 0.6277 | Val Loss: 0.6485 | LR: 0.00100\n",
      "Epoch 23, Loss: 0.6271\n",
      "Epoch 23 | Train Loss: 0.6271 | Val Loss: 0.6499 | LR: 0.00100\n",
      "Epoch 24, Loss: 0.6259\n",
      "Epoch 24 | Train Loss: 0.6259 | Val Loss: 0.6489 | LR: 0.00100\n",
      "Epoch 25, Loss: 0.6230\n",
      "Epoch 25 | Train Loss: 0.6230 | Val Loss: 0.6477 | LR: 0.00100\n",
      "Epoch 26, Loss: 0.6223\n",
      "Epoch 26 | Train Loss: 0.6223 | Val Loss: 0.6503 | LR: 0.00100\n",
      "Epoch 27, Loss: 0.6222\n",
      "Epoch 27 | Train Loss: 0.6222 | Val Loss: 0.6464 | LR: 0.00100\n",
      "Epoch 28, Loss: 0.6188\n",
      "Epoch 28 | Train Loss: 0.6188 | Val Loss: 0.6462 | LR: 0.00100\n",
      "Epoch 29, Loss: 0.6201\n",
      "Epoch 29 | Train Loss: 0.6201 | Val Loss: 0.6460 | LR: 0.00100\n",
      "Epoch 30, Loss: 0.6180\n",
      "Epoch 30 | Train Loss: 0.6180 | Val Loss: 0.6467 | LR: 0.00100\n",
      "Epoch 31, Loss: 0.6167\n",
      "Epoch 31 | Train Loss: 0.6167 | Val Loss: 0.6451 | LR: 0.00100\n",
      "Epoch 32, Loss: 0.6159\n",
      "Epoch 32 | Train Loss: 0.6159 | Val Loss: 0.6419 | LR: 0.00100\n",
      "Epoch 33, Loss: 0.6148\n",
      "Epoch 33 | Train Loss: 0.6148 | Val Loss: 0.6419 | LR: 0.00100\n",
      "Epoch 34, Loss: 0.6130\n",
      "Epoch 34 | Train Loss: 0.6130 | Val Loss: 0.6437 | LR: 0.00100\n",
      "Epoch 35, Loss: 0.6125\n",
      "Epoch 35 | Train Loss: 0.6125 | Val Loss: 0.6410 | LR: 0.00100\n",
      "Epoch 36, Loss: 0.6121\n",
      "Epoch 36 | Train Loss: 0.6121 | Val Loss: 0.6399 | LR: 0.00100\n",
      "Epoch 37, Loss: 0.6132\n",
      "Epoch 37 | Train Loss: 0.6132 | Val Loss: 0.6426 | LR: 0.00100\n",
      "Epoch 38, Loss: 0.6130\n",
      "Epoch 38 | Train Loss: 0.6130 | Val Loss: 0.6414 | LR: 0.00100\n",
      "Epoch 39, Loss: 0.6111\n",
      "Epoch 39 | Train Loss: 0.6111 | Val Loss: 0.6407 | LR: 0.00100\n",
      "Epoch 40, Loss: 0.6097\n",
      "Epoch 40 | Train Loss: 0.6097 | Val Loss: 0.6407 | LR: 0.00100\n",
      "Epoch 41, Loss: 0.6083\n",
      "Epoch 41 | Train Loss: 0.6083 | Val Loss: 0.6424 | LR: 0.00100\n",
      "Epoch 42, Loss: 0.6078\n",
      "Epoch 42 | Train Loss: 0.6078 | Val Loss: 0.6400 | LR: 0.00100\n",
      "Epoch 43, Loss: 0.6083\n",
      "Epoch 43 | Train Loss: 0.6083 | Val Loss: 0.6414 | LR: 0.00100\n",
      "Epoch 44, Loss: 0.6071\n",
      "Epoch 44 | Train Loss: 0.6071 | Val Loss: 0.6430 | LR: 0.00100\n",
      "Epoch 45, Loss: 0.6068\n",
      "Epoch 45 | Train Loss: 0.6068 | Val Loss: 0.6403 | LR: 0.00100\n",
      "Epoch 46, Loss: 0.6036\n",
      "Epoch 46 | Train Loss: 0.6036 | Val Loss: 0.6415 | LR: 0.00100\n",
      "Epoch 47, Loss: 0.6041\n",
      "Epoch 47 | Train Loss: 0.6041 | Val Loss: 0.6411 | LR: 0.00100\n",
      "Epoch 48, Loss: 0.6047\n",
      "Epoch 48 | Train Loss: 0.6047 | Val Loss: 0.6388 | LR: 0.00100\n",
      "Epoch 49, Loss: 0.6047\n",
      "Epoch 49 | Train Loss: 0.6047 | Val Loss: 0.6434 | LR: 0.00100\n",
      "Epoch 50, Loss: 0.6057\n",
      "Epoch 50 | Train Loss: 0.6057 | Val Loss: 0.6409 | LR: 0.00100\n",
      "Epoch 51, Loss: 0.6049\n",
      "Epoch 51 | Train Loss: 0.6049 | Val Loss: 0.6399 | LR: 0.00100\n",
      "Epoch 52, Loss: 0.6018\n",
      "Epoch 52 | Train Loss: 0.6018 | Val Loss: 0.6382 | LR: 0.00100\n",
      "Epoch 53, Loss: 0.6018\n",
      "Epoch 53 | Train Loss: 0.6018 | Val Loss: 0.6379 | LR: 0.00100\n",
      "Epoch 54, Loss: 0.6015\n",
      "Epoch 54 | Train Loss: 0.6015 | Val Loss: 0.6394 | LR: 0.00100\n",
      "Epoch 55, Loss: 0.6025\n",
      "Epoch 55 | Train Loss: 0.6025 | Val Loss: 0.6431 | LR: 0.00100\n",
      "Epoch 56, Loss: 0.6001\n",
      "Epoch 56 | Train Loss: 0.6001 | Val Loss: 0.6391 | LR: 0.00100\n",
      "Epoch 57, Loss: 0.5988\n",
      "Epoch 57 | Train Loss: 0.5988 | Val Loss: 0.6405 | LR: 0.00100\n",
      "Epoch 58, Loss: 0.5985\n",
      "Epoch 58 | Train Loss: 0.5985 | Val Loss: 0.6417 | LR: 0.00100\n",
      "Epoch 59, Loss: 0.5999\n",
      "Epoch 59 | Train Loss: 0.5999 | Val Loss: 0.6399 | LR: 0.00100\n",
      "Epoch 60, Loss: 0.5982\n",
      "Epoch 60 | Train Loss: 0.5982 | Val Loss: 0.6380 | LR: 0.00100\n",
      "Epoch 61, Loss: 0.5993\n",
      "Epoch 61 | Train Loss: 0.5993 | Val Loss: 0.6386 | LR: 0.00100\n",
      "Epoch 62, Loss: 0.5967\n",
      "Epoch 62 | Train Loss: 0.5967 | Val Loss: 0.6382 | LR: 0.00100\n",
      "Epoch 63, Loss: 0.5932\n",
      "Epoch 63 | Train Loss: 0.5932 | Val Loss: 0.6460 | LR: 0.00100\n",
      "Epoch 64, Loss: 0.5977\n",
      "Epoch 64 | Train Loss: 0.5977 | Val Loss: 0.6401 | LR: 0.00100\n",
      "Epoch 65, Loss: 0.5959\n",
      "Epoch 65 | Train Loss: 0.5959 | Val Loss: 0.6395 | LR: 0.00100\n",
      "Epoch 66, Loss: 0.5972\n",
      "Epoch 66 | Train Loss: 0.5972 | Val Loss: 0.6429 | LR: 0.00100\n",
      "Epoch 67, Loss: 0.5974\n",
      "Epoch 67 | Train Loss: 0.5974 | Val Loss: 0.6399 | LR: 0.00100\n",
      "Epoch 68, Loss: 0.5953\n",
      "Epoch 68 | Train Loss: 0.5953 | Val Loss: 0.6407 | LR: 0.00100\n",
      "Epoch 69, Loss: 0.5963\n",
      "Epoch 69 | Train Loss: 0.5963 | Val Loss: 0.6380 | LR: 0.00100\n",
      "Epoch 70, Loss: 0.5938\n",
      "Epoch 70 | Train Loss: 0.5938 | Val Loss: 0.6404 | LR: 0.00100\n",
      "Epoch 71, Loss: 0.5934\n",
      "Epoch 71 | Train Loss: 0.5934 | Val Loss: 0.6346 | LR: 0.00100\n",
      "Epoch 72, Loss: 0.5921\n",
      "Epoch 72 | Train Loss: 0.5921 | Val Loss: 0.6389 | LR: 0.00100\n",
      "Epoch 73, Loss: 0.5922\n",
      "Epoch 73 | Train Loss: 0.5922 | Val Loss: 0.6379 | LR: 0.00100\n",
      "Epoch 74, Loss: 0.5951\n",
      "Epoch 74 | Train Loss: 0.5951 | Val Loss: 0.6381 | LR: 0.00100\n",
      "Epoch 75, Loss: 0.5944\n",
      "Epoch 75 | Train Loss: 0.5944 | Val Loss: 0.6422 | LR: 0.00100\n",
      "Epoch 76, Loss: 0.5932\n",
      "Epoch 76 | Train Loss: 0.5932 | Val Loss: 0.6384 | LR: 0.00100\n",
      "Epoch 77, Loss: 0.5913\n",
      "Epoch 77 | Train Loss: 0.5913 | Val Loss: 0.6417 | LR: 0.00100\n",
      "Epoch 78, Loss: 0.5910\n",
      "Epoch 78 | Train Loss: 0.5910 | Val Loss: 0.6416 | LR: 0.00100\n",
      "Epoch 79, Loss: 0.5955\n",
      "Epoch 79 | Train Loss: 0.5955 | Val Loss: 0.6401 | LR: 0.00100\n",
      "Epoch 80, Loss: 0.5923\n",
      "Epoch 80 | Train Loss: 0.5923 | Val Loss: 0.6353 | LR: 0.00100\n",
      "Epoch 81, Loss: 0.5904\n",
      "Epoch 81 | Train Loss: 0.5904 | Val Loss: 0.6336 | LR: 0.00100\n",
      "Epoch 82, Loss: 0.5921\n",
      "Epoch 82 | Train Loss: 0.5921 | Val Loss: 0.6389 | LR: 0.00100\n",
      "Epoch 83, Loss: 0.5921\n",
      "Epoch 83 | Train Loss: 0.5921 | Val Loss: 0.6360 | LR: 0.00100\n",
      "Epoch 84, Loss: 0.5900\n",
      "Epoch 84 | Train Loss: 0.5900 | Val Loss: 0.6423 | LR: 0.00100\n",
      "Epoch 85, Loss: 0.5928\n",
      "Epoch 85 | Train Loss: 0.5928 | Val Loss: 0.6390 | LR: 0.00100\n",
      "Epoch 86, Loss: 0.5914\n",
      "Epoch 86 | Train Loss: 0.5914 | Val Loss: 0.6455 | LR: 0.00100\n",
      "Epoch 87, Loss: 0.5902\n",
      "Epoch 87 | Train Loss: 0.5902 | Val Loss: 0.6394 | LR: 0.00100\n",
      "Epoch 88, Loss: 0.5900\n",
      "Epoch 88 | Train Loss: 0.5900 | Val Loss: 0.6377 | LR: 0.00100\n",
      "Epoch 89, Loss: 0.5922\n",
      "Epoch 89 | Train Loss: 0.5922 | Val Loss: 0.6358 | LR: 0.00100\n",
      "Epoch 90, Loss: 0.5912\n",
      "Epoch 90 | Train Loss: 0.5912 | Val Loss: 0.6356 | LR: 0.00100\n",
      "Epoch 91, Loss: 0.5893\n",
      "Epoch 91 | Train Loss: 0.5893 | Val Loss: 0.6355 | LR: 0.00100\n",
      "Epoch 92, Loss: 0.5891\n",
      "Epoch 92 | Train Loss: 0.5891 | Val Loss: 0.6375 | LR: 0.00100\n",
      "Epoch 93, Loss: 0.5893\n",
      "Epoch 93 | Train Loss: 0.5893 | Val Loss: 0.6372 | LR: 0.00100\n",
      "Epoch 94, Loss: 0.5882\n",
      "Epoch 94 | Train Loss: 0.5882 | Val Loss: 0.6371 | LR: 0.00100\n",
      "Epoch 95, Loss: 0.5877\n",
      "Epoch 95 | Train Loss: 0.5877 | Val Loss: 0.6401 | LR: 0.00100\n",
      "Epoch 96, Loss: 0.5902\n",
      "Epoch 96 | Train Loss: 0.5902 | Val Loss: 0.6352 | LR: 0.00100\n",
      "Epoch 97, Loss: 0.5882\n",
      "Epoch 97 | Train Loss: 0.5882 | Val Loss: 0.6384 | LR: 0.00100\n",
      "Epoch 98, Loss: 0.5876\n",
      "Epoch 98 | Train Loss: 0.5876 | Val Loss: 0.6368 | LR: 0.00100\n",
      "Epoch 99, Loss: 0.5871\n",
      "Epoch 99 | Train Loss: 0.5871 | Val Loss: 0.6362 | LR: 0.00100\n",
      "Epoch 100, Loss: 0.5867\n",
      "Epoch 100 | Train Loss: 0.5867 | Val Loss: 0.6355 | LR: 0.00100\n",
      "Test Accuracy: 62.58%\n"
     ]
    }
   ],
   "source": [
    "# === 4. –û–±—É—á–µ–Ω–∏–µ ===\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X1, X2, y, match_feats in train_loader:\n",
    "        X1, X2, y, match_feats = X1, X2, y.float(), match_feats\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X1, X2, match_feats)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X1, X2, y, match_feats in test_loader:\n",
    "            outputs = model(X1, X2, match_feats)\n",
    "            loss = criterion(outputs, y.float())\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "\n",
    "    #scheduler.step(avg_val_loss)  # üîë –æ–±–Ω–æ–≤–ª—è–µ–º lr\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:02d} | \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "        f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "        f\"LR: {optimizer.param_groups[0]['lr']:.5f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820a330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 74.39%\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for X1, X2, y, match_feats in test_loader:\n",
    "        outputs = model(X1, X2, match_feats)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs > 0.5).long()\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {correct/total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a85039c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 64.61%\n"
     ]
    }
   ],
   "source": [
    "#Transformers\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for X1, X2, y, match_feats in test_loader:\n",
    "        outputs = model(X1, X2, match_feats)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = (probs > 0.5).long()\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {correct/total:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
